{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Demo 2.2 - Model Reliability & Enhancing LLMs\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dair-ai/maven-pe-for-llms/blob/main/notebooks/session-2/demo-2.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install --upgrade chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "import openai\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "import IPython\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting token usage\n",
    "\n",
    "We will use `tiktoken`, an open-source tokenizer by OpenAI.\n",
    "\n",
    "https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoding by name\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# load the correct encoding by passing the model name\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1097, 8430, 6380, 3432]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize text\n",
    "encoding.encode(\"I am feeling happy today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count tokens\n",
    "len(encoding.encode(\"I am feeling happy today\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gpt-4 model\n",
    "gpt4_encoding = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1097, 8430, 6380, 3432]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_encoding.encode(\"I am feeling happy today\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate token usage with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 198\n",
      "\tPrompt Tokens: 8\n",
      "\tCompletion Tokens: 190\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.003960000000000001\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# anything inside the context manager will be tracked\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a short story about a robot\")\n",
    "    print(cb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be clear and specific when prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I couldn't find a movie to recommend today.\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommend movies to a customer. \n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}. \n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"\n",
    "Please recommend a movie based on my interests.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example where the customer provides information about interests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your interest in super-hero movies, I recommend you watch \"Spider-Man: No Way Home\". It is one of the top global trending movies and features the beloved superhero Spider-Man. Enjoy the movie!\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommends movies to a customer. \n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}. \n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"\n",
    "I love super-hero movies. Please recommend a movie based on my interests.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Delimiters to Distinguish Components of a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```pythonstrings2 = []\n",
       "strings2.append(\"one\")\n",
       "strings2.append(\"two\")\n",
       "strings2.append(\"THREE\")\n",
       "strings2.append(\"4\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Convert the following code block in the #### <code> #### section to Python:\n",
    "\n",
    "####\n",
    "strings2.push(\"one\")\n",
    "strings2.push(\"two\")\n",
    "strings2.push(\"THREE\")\n",
    "strings2.push(\"4\")\n",
    "####\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "IPython.display.Markdown(\"```python\" + get_completion(message) + \"\\n```\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"product_name\": \"Nike Air Max 270 React\",\n",
      "  \"product_brand\": \"Nike\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
    "\n",
    "Product Description: Introducing the Nike Air Max 270 React: a comfortable and stylish sneaker that combines two of Nike's best technologies. With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
    "\n",
    "###\n",
    "product_name: the name of the product\n",
    "product_bran: the name of the brand (if any) \n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Length of the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear [Customer], we have located your package and it is on its way to you. We apologize for any inconvenience caused and have taken steps to prevent similar issues in the future. Thank you for your understanding.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is: given a customer support email, which is delimited with ###, generate a shorter 1-2 sentence response.\n",
    "\n",
    "###\n",
    "Dear [Customer],\n",
    "\n",
    "We hope this email finds you well. We wanted to update you on the shipping issue you experienced with your recent order. After investigating the issue, we have located your package and it is currently on its way to you. We apologize for any inconvenience this may have caused and thank you for your patience and understanding while we resolved this matter.\n",
    "\n",
    "Please note that we have taken steps to prevent similar issues from occurring in the future. We have improved our shipping tracking system and are now better equipped to ensure that packages arrive on time and in good condition. We take the quality of our service very seriously and want to ensure that all of our customers have a positive experience when shopping with us.\n",
    "\n",
    "Once again, we apologize for any inconvenience this may have caused and hope that you will continue to shop with us in the future. If you have any further questions or concerns, please do not hesitate to contact us. We are always here to help and ensure that your shopping experience is a positive one.\n",
    "\n",
    "Thank you for your understanding.\n",
    "\n",
    "Best regards,\n",
    "\n",
    "[Your Name]\n",
    "\n",
    "Customer Support Team\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {  \n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt       \n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid deviating; Constrain the Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it helps to be more specific about what output you expect to avoid the model deviating from the main task of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would recommend watching \"The Shawshank Redemption.\" It is a highly acclaimed drama film that tells the story of a banker who is sentenced to life in Shawshank State Penitentiary for a crime he did not commit. The movie explores themes of hope, friendship, and the resilience of the human spirit. It has a compelling storyline, brilliant performances, and is considered one of the greatest films of all time.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Recommend a movie for Saturday:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Recommend a movie for Saturday. Just say the movie, no need for explanations!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Task into Subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: July 15th to July 16th\n",
      "Event: Summer Beats Festival\n"
     ]
    }
   ],
   "source": [
    "event = \"\"\"\n",
    "Summer Beats Festival\n",
    "\n",
    "The event will be held at the beautiful seaside location of Ocean Park in Miami, Florida.\n",
    "\n",
    "The festival will take place over two days, from July 15th to July 16th.\n",
    "\n",
    "The Summer Beats Festival will feature a fantastic lineup of popular musical artists and bands from a variety of genres. Attendees can expect to dance and sing along to live performances from headliners such as Taylor Swift, Bruno Mars, and Post Malone. In addition to the main stage, there will be several smaller stages scattered throughout the park featuring up-and-coming artists and DJs.\n",
    "\n",
    "The festival will also offer a wide variety of food and drink options for attendees to enjoy. From classic festival fare like hot dogs and funnel cakes to more gourmet offerings like sushi and craft beer, there will be something to suit every taste.\n",
    "\n",
    "Families with children are welcome, and there will be plenty of activities to keep the little ones entertained. The festival will offer a dedicated children's area with carnival games, face painting, and other fun activities.\n",
    "\n",
    "For those looking for a more luxurious experience, the Summer Beats Festival will also offer a VIP area with premium viewing of the main stage, private bars, and lounges, and other exclusive perks.\n",
    "\n",
    "Overall, the Summer Beats Festival promises to be an unforgettable event for music lovers of all ages. With a stunning location, a great lineup of artists, and plenty of activities and amenities, it's sure to be the highlight of the summer!\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Your task is to extract the date of the event and the name of the event. The event is delimited by ### ###.\n",
    "\n",
    "###\n",
    "Event: {event}\n",
    "###\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt.format(event=event)\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you add more tasks within the prompt you need to me more detailed and specific with the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event is the Summer Beats Festival, which will be held at Ocean Park in Miami, Florida. The festival will take place over two days, from July 15th to July 16th.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Explain the event in 2 sentences. \n",
    "Extract the date of the event and the name of the event. The event is delimited by ### ###. \n",
    "\n",
    "Transform the dates into a MM/DD.\n",
    "\n",
    "###\n",
    "Event: {event}\n",
    "###\n",
    "\n",
    "Output format: Explanation | event name | date\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt.format(event=event)\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above is not exactly what I wanted. The way to fix it is by being very specific and spell out the steps in details. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling\n",
    "\n",
    "A useful function to get reliable outputs and format responses that can interact with external tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(messages, functions, function_call, model=\"gpt-3.5-turbo-0613\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        functions = functions,\n",
    "        function_call=function_call,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"\"\"\n",
    "Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_summary_and_tags\",\n",
    "        \"description\": \"Returns the summary and tags of a given text.\",\n",
    "        \"parameters\": { # arguments of our function that ChatGPT will send to use and what type\n",
    "            \"type\": \"object\", # we want arguments as objects\n",
    "            \"properties\": { # properties of the object\n",
    "                \"tags\": {\n",
    "                    \"type\": \"string\", # tags are a string\n",
    "                    \"description\": \"The tags correspond to the machine learning models present in the text.\"\n",
    "                },\n",
    "                \"summary\": {\n",
    "                    \"type\": \"string\", # summary is a string\n",
    "                    \"description\": \"The summary of the text output.\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Your task is to extract the summary and tags of the following text.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Here is the text: {abstract}. Now return the summary and tags.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "response = get_completion(messages, functions, {\"name\": \"get_summary_and_tags\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7WcxoPqQpllAB4cnFYKU5bmSAQ5s5\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688010628,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_summary_and_tags\",\n",
      "          \"arguments\": \"{\\n  \\\"summary\\\": \\\"Training large language models with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. In this paper, the authors propose a method to create large amounts of instruction data using language models instead of humans. They show that the instructions generated by their approach are superior to human-created ones. The authors suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models.\\\",\\n  \\\"tags\\\": \\\"large language models, open-domain instruction, data creation, complexity, Evol-Instruct, WizardLM, fine-tuning, AI-evolved instructions\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 330,\n",
      "    \"completion_tokens\": 132,\n",
      "    \"total_tokens\": 462\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "final_object = json.loads(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Training large language models with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. In this paper, the authors propose a method to create large amounts of instruction data using language models instead of humans. They show that the instructions generated by their approach are superior to human-created ones. The authors suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models.',\n",
       " 'tags': 'large language models, open-domain instruction, data creation, complexity, Evol-Instruct, WizardLM, fine-tuning, AI-evolved instructions'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can then be the information to be passed to something like an API. The API then responds back with some information that you can then pass back to GPT to compose a reply if it was a chatbot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
